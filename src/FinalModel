import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from pytorch_tabnet.tab_model import TabNetClassifier
import torch
import math
import warnings
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# ============== 1) 加载 WDBC 数据集 ==============
columns = [
    'ID', 'Diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',
    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',
    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
    'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',
    'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',
    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',
    'fractal_dimension_worst'
]

file_path = r"E:\google\Dataset\breast cancer\breast cancer\wdbc.data"
df = pd.read_csv(file_path, header=None, names=columns)

df['Diagnosis'] = LabelEncoder().fit_transform(df['Diagnosis'])
X = df.drop(['ID', 'Diagnosis'], axis=1).values
y = df['Diagnosis'].values

# 标准化
scaler = RobustScaler()
X = scaler.fit_transform(X)

# ============== 2) 参数设置 ==============
SEED = 42
n_splits = 5  # 外层交叉验证的折数
torch.manual_seed(SEED)
np.random.seed(SEED)
outer_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)

# 特征工程参数
pca_components = 8  # PCA降维后的维度
feature_select_k = 101  # 特征选择保留的特征数量

results_list = []
fold = 1

# ============== 3) 深度特征提取函数 ==============
def extract_deep_features(tabnet_model, X_data):
    tabnet_model.network.eval()
    with torch.no_grad():
        X_tensor = torch.tensor(X_data).float()
        output, M_loss = tabnet_model.network.forward_masks(X_tensor)
        final_embedding = output.cpu().numpy()
        feature_importance = np.var(final_embedding, axis=0)
        feature_importance = feature_importance / np.sum(feature_importance)
        n_features = X_data.shape[1]
        if len(feature_importance) < n_features:
            repeat_times = n_features // len(feature_importance) + 1
            attention_weights = np.tile(feature_importance, repeat_times)[:n_features]
        else:
            attention_weights = feature_importance[:n_features]
        weighted_original = X_data * attention_weights
        multi_view_features = []
        for i in range(tabnet_model.n_steps):
            view_weights = attention_weights * (i + 1) / tabnet_model.n_steps
            view_features = X_data * view_weights
            multi_view_features.append(view_features)
        all_steps_features = np.concatenate(multi_view_features, axis=1)
        

        
    return final_embedding, all_steps_features, attention_weights, weighted_original

# ============== 4) 外层交叉验证循环 ==============
for train_idx, test_idx in outer_kf.split(X, y):
    print(f"\n===== Fold {fold} =====")
    X_train_full, X_test = X[train_idx], X[test_idx]
    y_train_full, y_test = y[train_idx], y[test_idx]

    # ========== 内层 OOF stacking ==========
    inner_kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)
    oof_preds = []
    oof_targets = []
    test_preds = []

    for inner_fold, (inner_tr_idx, inner_va_idx) in enumerate(inner_kf.split(X_train_full, y_train_full)):
        X_tr, X_va = X_train_full[inner_tr_idx], X_train_full[inner_va_idx]
        y_tr, y_va = y_train_full[inner_tr_idx], y_train_full[inner_va_idx]

        # ---- TabNet ----
        batch_size = 128
        steps_per_epoch = max(1, math.ceil(X_tr.shape[0] / batch_size))
        tabnet = TabNetClassifier(
            n_d=32, n_a=32, n_steps=4,
            gamma=1.8,
            lambda_sparse=5e-4,
            clip_value=2.0,
            mask_type="entmax",
            optimizer_fn=torch.optim.AdamW,
            optimizer_params=dict(lr=1e-3, weight_decay=1e-4),
            scheduler_params={"max_lr": 3e-3, "epochs": 200, "steps_per_epoch": steps_per_epoch, "pct_start": 0.2},
            scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,
            seed=SEED, verbose=0
        )
        tabnet.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            eval_name=["valid"],
            eval_metric=["auc"],
            patience=20,
            max_epochs=500,
            batch_size=batch_size,
            virtual_batch_size=64,
            num_workers=0,
            drop_last=False
        )

        # ---- 特征提取与融合 ----
        fe_tr, st_tr, aw_tr, wo_tr = extract_deep_features(tabnet, X_tr)
        fe_va, st_va, aw_va, wo_va = extract_deep_features(tabnet, X_va)
        fe_te, st_te, aw_te, wo_te = extract_deep_features(tabnet, X_test)

        # 限制PCA维度
        pca = PCA(n_components=pca_components, random_state=SEED)
        st_tr_pca = pca.fit_transform(st_tr)
        st_va_pca = pca.transform(st_va)
        st_te_pca = pca.transform(st_te)

        # 为XGBoost构建特征集V1: 原始特征 + 最终embedding + 所有步骤特征
        hybrid_tr_xgb = np.concatenate([X_tr, fe_tr, st_tr_pca], axis=1)
        hybrid_va_xgb = np.concatenate([X_va, fe_va, st_va_pca], axis=1)
        hybrid_te_xgb = np.concatenate([X_test, fe_te, st_te_pca], axis=1)
        
        # 限制XGBoost特征维度
        if hybrid_tr_xgb.shape[1] > feature_select_k:
            from sklearn.feature_selection import SelectKBest, f_classif
            selector_xgb = SelectKBest(f_classif, k=feature_select_k)
            hybrid_tr_xgb = selector_xgb.fit_transform(hybrid_tr_xgb, y_tr)
            hybrid_va_xgb = selector_xgb.transform(hybrid_va_xgb)
            hybrid_te_xgb = selector_xgb.transform(hybrid_te_xgb)
        
        # 为LightGBM构建特征集V2: 注意力加权特征 + embedding
        hybrid_tr_lgb = np.concatenate([wo_tr, fe_tr], axis=1)
        hybrid_va_lgb = np.concatenate([wo_va, fe_va], axis=1)
        hybrid_te_lgb = np.concatenate([wo_te, fe_te], axis=1)
        


        # ---- XGB (使用特征集V1) ----
        xgb_clf = xgb.XGBClassifier(
            n_estimators=569,  # 降低n_estimators
            max_depth=5,
            learning_rate=0.34155,
            subsample=0.74263,
            colsample_bytree=0.738128,
            random_state=SEED,
            eval_metric='auc',
            early_stopping_rounds=30  # 调整早停参数
        )
        xgb_clf.fit(hybrid_tr_xgb, y_tr, eval_set=[(hybrid_va_xgb, y_va)], verbose=False)
        val_proba_xgb = xgb_clf.predict_proba(hybrid_va_xgb)[:, 1]
        test_proba_xgb = xgb_clf.predict_proba(hybrid_te_xgb)[:, 1]

        # ---- LGB (使用特征集V2) ----
        lgb_train = lgb.Dataset(hybrid_tr_lgb, label=y_tr)
        lgb_val_dataset = lgb.Dataset(hybrid_va_lgb, label=y_va, reference=lgb_train)
        params = {
            "objective": "binary",
            "boosting_type": "gbdt",
            "metric": "auc",
            "num_leaves": 146,
            "learning_rate": 0.0773,
            "subsample": 0.7682,
            "colsample_bytree": 0.77594,
            "reg_alpha": 0.6171,
            "reg_lambda": 0.2901,
            "min_child_weight": 4,
            "random_state": SEED,
            "verbose": -1
        }
        lgb_clf = lgb.train(
            params,
            lgb_train,
            valid_sets=[lgb_val_dataset],
            num_boost_round=2000,
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        val_proba_lgb = lgb_clf.predict(hybrid_va_lgb, num_iteration=lgb_clf.best_iteration)
        test_proba_lgb = lgb_clf.predict(hybrid_te_lgb, num_iteration=lgb_clf.best_iteration)

        # ---- 保存 OOF ----
        stack_val = np.vstack([val_proba_xgb, val_proba_lgb]).T
        stack_test = np.vstack([test_proba_xgb, test_proba_lgb]).T

        oof_preds.append(stack_val)
        oof_targets.append(y_va)
        test_preds.append(stack_test)
        
        # 保存最后一折的特征信息用于后续分析
        if inner_fold == n_splits - 1:
            last_fold_features = {
                'fe_te': fe_te, 'st_te': st_te, 'aw_te': aw_te, 'wo_te': wo_te,
                'st_te_pca': st_te_pca, 'hybrid_te_xgb': hybrid_te_xgb, 
                'hybrid_te_lgb': hybrid_te_lgb, 'X_test': X_test
            }

    # 确保last_fold_features已定义
    if 'last_fold_features' not in locals():
        # 如果没有执行到最后一折，创建一个默认的last_fold_features
        last_fold_features = {
            'fe_te': fe_te, 'st_te': st_te, 'aw_te': aw_te, 'wo_te': wo_te,
            'st_te_pca': st_te_pca, 'hybrid_te_xgb': hybrid_te_xgb, 
            'hybrid_te_lgb': hybrid_te_lgb, 'X_test': X_test
        }
    
    # ====== Meta 模型训练 ======
    oof_preds = np.vstack(oof_preds)
    oof_targets = np.concatenate(oof_targets)
    test_preds = np.mean(test_preds, axis=0)

    meta_clf = LogisticRegression(random_state=SEED, solver="liblinear")
    meta_clf.fit(oof_preds, oof_targets)

    y_test_proba = meta_clf.predict_proba(test_preds)[:, 1]
    thresholds = np.linspace(0.1, 0.9, 81)
    best_youden_j = 0
    best_thr = 0.5
    for thr in thresholds:
        y_pred = (y_test_proba >= thr).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        youden_j = sens + spec - 1
        if youden_j > best_youden_j:
            best_youden_j = youden_j
            best_thr = thr

    y_pred_final = (y_test_proba >= best_thr).astype(int)
    acc = accuracy_score(y_test, y_pred_final)
    f1 = f1_score(y_test, y_pred_final)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final).ravel()
    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # ============== 特征使用情况分析 ==============
    print(f"\n=== Fold {fold} - TabNet特征提取与模型特征集分析 ===")
    print(f"基于内层交叉验证最后一折的特征构成:")
    
    # 显示TabNet输出的三层特征信息
    print(f"\n1. TabNet三层特征输出:")
    print(f"   - 最终嵌入特征 (final_embedding): {last_fold_features['fe_te'].shape} -> 用于XGB和LGB")
    print(f"   - 多步骤特征 (all_steps_features): {last_fold_features['st_te'].shape} -> 经PCA降维后用于XGB")
    print(f"   - 注意力权重 (attention_weights): {last_fold_features['aw_te'].shape} -> 用于生成加权特征")
    print(f"   - 加权原始特征 (weighted_original): {last_fold_features['wo_te'].shape} -> 用于LGB")
    
    # 显示后续模型使用的特征集
    print(f"\n2. XGBoost特征集V1构成 (总维度: {last_fold_features['hybrid_te_xgb'].shape[1]}):")
    print(f"   - 原始特征: {last_fold_features['X_test'].shape[1]}维")
    print(f"   - TabNet嵌入: {last_fold_features['fe_te'].shape[1]}维")
    print(f"   - PCA降维步骤特征: {last_fold_features['st_te_pca'].shape[1]}维")
    if last_fold_features['hybrid_te_xgb'].shape[1] != (last_fold_features['X_test'].shape[1] + last_fold_features['fe_te'].shape[1] + last_fold_features['st_te_pca'].shape[1]):
        print(f"   - 特征选择后: {last_fold_features['hybrid_te_xgb'].shape[1]}维 (SelectKBest保留前{feature_select_k}个)")
    
    print(f"\n3. LightGBM特征集V2构成 (总维度: {last_fold_features['hybrid_te_lgb'].shape[1]}):")
    print(f"   - 注意力加权特征: {last_fold_features['wo_te'].shape[1]}维")
    print(f"   - TabNet嵌入: {last_fold_features['fe_te'].shape[1]}维")
    print(f"   - 无需特征选择 (维度 < {feature_select_k})")
    
    print(f"\n4. 注意力权重分布:")
    print(f"   - 权重范围: [{last_fold_features['aw_te'].min():.6f}, {last_fold_features['aw_te'].max():.6f}]")
    print(f"   - 前10个特征权重: {last_fold_features['aw_te'][:10]}")
    
    # ============== SHAP 分析 ==============
    print(f"\n--- SHAP Analysis for Fold {fold} ---")
    print(f"分析说明: 使用内部交叉验证最后一个fold训练的实际模型进行SHAP分析")
    
    # 定义原始特征名称
    original_feature_names = [
        'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
        'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',
        'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',
        'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
        'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',
        'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',
        'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',
        'fractal_dimension_worst'
    ]
    
    # 1. 元模型 SHAP 分析
    try:
        # 使用线性解释器分析元模型（LogisticRegression）
        explainer_meta = shap.LinearExplainer(meta_clf, oof_preds)
        shap_values_meta = explainer_meta.shap_values(test_preds)
        
        print(f"Meta Model SHAP - Base Model Contributions:")
        print(f"  XGBoost contribution: {np.mean(np.abs(shap_values_meta[:, 0])):.4f}")
        print(f"  LightGBM contribution: {np.mean(np.abs(shap_values_meta[:, 1])):.4f}")
        
        # 保存元模型SHAP图
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values_meta, test_preds, 
                         feature_names=['XGBoost_Probability', 'LightGBM_Probability'], 
                         show=False)
        plt.title(f'Meta Model SHAP Analysis - Fold {fold}\n(Stacking Ensemble Decision Process)')
        plt.tight_layout()
        plt.savefig(f'meta_model_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"Meta model SHAP analysis failed: {e}")
    
    # 2. XGBoost SHAP 分析（使用内部循环最后一个fold的实际模型）
    try:
        print(f"\n=== XGBoost Model Analysis (Inner CV Last Fold) ===")
        print(f"模型配置: n_estimators=569, max_depth=5, learning_rate=0.34155")
        print(f"特征集V1: 原始特征(30) + TabNet嵌入(32) + PCA降维步骤特征(8) = 总计70维")
        print(f"特征选择: 使用SelectKBest保留前{feature_select_k}个最重要特征")
        
        # 使用内部循环最后一个fold的模型进行SHAP分析
        # 重新训练以获得与内部循环相同的模型配置
        final_xgb = xgb.XGBClassifier(
            n_estimators=569, max_depth=5, learning_rate=0.34155,
            subsample=0.74263, colsample_bytree=0.738128,
            random_state=SEED, eval_metric='auc'
        )
        
        # 使用完整训练集训练最终模型（模拟内部循环最后一个fold）
        X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
            X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=SEED
        )
        
        # 重新提取特征用于SHAP分析
        tabnet_final = TabNetClassifier(
            n_d=32, n_a=32, n_steps=4, gamma=1.8, lambda_sparse=5e-4,
            clip_value=2.0, mask_type="entmax", optimizer_fn=torch.optim.AdamW,
            optimizer_params=dict(lr=1e-3, weight_decay=1e-4),
            seed=SEED, verbose=0
        )
        tabnet_final.fit(X_train_final, y_train_final, eval_set=[(X_val_final, y_val_final)],
                        eval_name=["valid"], eval_metric=["auc"], patience=20,
                        max_epochs=200, batch_size=128, virtual_batch_size=64,
                        num_workers=0, drop_last=False)
        
        fe_final, st_final, _, _ = extract_deep_features(tabnet_final, X_train_final)
        fe_test_final, st_test_final, _, _ = extract_deep_features(tabnet_final, X_test)
        
        pca_final = PCA(n_components=pca_components, random_state=SEED)
        st_final_pca = pca_final.fit_transform(st_final)
        st_test_final_pca = pca_final.transform(st_test_final)
        
        # 构建XGBoost特征集V1的特征名称
        tabnet_embedding_names = [f'TabNet_Embed_{i+1}' for i in range(fe_final.shape[1])]
        pca_step_names = [f'TabNet_PCA_Step_{i+1}' for i in range(st_final_pca.shape[1])]
        xgb_feature_names = original_feature_names + tabnet_embedding_names + pca_step_names
        
        hybrid_final_xgb = np.concatenate([X_train_final, fe_final, st_final_pca], axis=1)
        hybrid_test_final_xgb = np.concatenate([X_test, fe_test_final, st_test_final_pca], axis=1)
        
        # 特征选择并保存选中的特征名称
        selected_feature_names = xgb_feature_names.copy()
        if hybrid_final_xgb.shape[1] > feature_select_k:
            from sklearn.feature_selection import SelectKBest, f_classif
            selector_final = SelectKBest(f_classif, k=feature_select_k)
            hybrid_final_xgb = selector_final.fit_transform(hybrid_final_xgb, y_train_final)
            hybrid_test_final_xgb = selector_final.transform(hybrid_test_final_xgb)
            # 获取选中特征的索引和名称
            selected_indices = selector_final.get_support(indices=True)
            selected_feature_names = [xgb_feature_names[i] for i in selected_indices]
        
        final_xgb.fit(hybrid_final_xgb, y_train_final)
        
        # XGBoost SHAP 分析
        explainer_xgb = shap.TreeExplainer(final_xgb)
        shap_values_xgb = explainer_xgb.shap_values(hybrid_test_final_xgb[:20])  # 只分析前20个样本以节省时间
        
        print(f"\nXGBoost Model - Top 10 Important Features (by mean |SHAP|):")
        feature_importance_xgb = np.mean(np.abs(shap_values_xgb), axis=0)
        top_features_idx = np.argsort(feature_importance_xgb)[-10:]
        for i, idx in enumerate(reversed(top_features_idx)):
            feature_name = selected_feature_names[idx] if idx < len(selected_feature_names) else f"Feature_{idx}"
            print(f"  {i+1:2d}. {feature_name}: {feature_importance_xgb[idx]:.4f}")
        
        # 保存XGBoost SHAP图
        plt.figure(figsize=(14, 10))
        shap.summary_plot(shap_values_xgb, hybrid_test_final_xgb[:20], 
                         feature_names=selected_feature_names,
                         max_display=15, show=False)
        plt.title(f'XGBoost SHAP Analysis - Fold {fold}\n(Feature Set V1: Original + TabNet Embeddings + PCA Steps)')
        plt.tight_layout()
        plt.savefig(f'xgboost_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"XGBoost SHAP analysis failed: {e}")
    
    # 3. LightGBM SHAP 分析（使用内部循环最后一个fold的实际模型）
    try:
        print(f"\n=== LightGBM Model Analysis (Inner CV Last Fold) ===")
        print(f"模型配置: num_leaves=146, learning_rate=0.0773, subsample=0.7682")
        print(f"特征集V2: 注意力加权特征(30) + TabNet嵌入(32) = 总计62维")
        print(f"特征选择: 使用SelectKBest保留前{feature_select_k}个最重要特征（如果超过62维）")
        
        # 使用与内部循环相同的LightGBM参数配置
        final_lgb = lgb.LGBMClassifier(
            num_leaves=146, learning_rate=0.0773, subsample=0.7682,
            colsample_bytree=0.77594, reg_alpha=0.6171, reg_lambda=0.2901,
            min_child_weight=4, random_state=SEED, objective='binary', 
            metric='auc', verbose=-1, n_estimators=2000
        )
        
        # 构建LightGBM特征集V2的特征名称（注意：使用注意力加权特征而非原始特征）
        _, _, aw_final, wo_final = extract_deep_features(tabnet_final, X_train_final)
        _, _, aw_test_final, wo_test_final = extract_deep_features(tabnet_final, X_test)
        
        # 为LightGBM构建特征集V2: 注意力加权特征 + embedding（与stacking中完全一致）
        weighted_feature_names = [f'Weighted_{name}' for name in original_feature_names]
        tabnet_embedding_names_lgb = [f'TabNet_Embed_{i+1}' for i in range(fe_final.shape[1])]
        lgb_feature_names = weighted_feature_names + tabnet_embedding_names_lgb
        
        # 特征集V2构建：wo_final(注意力加权特征30维) + fe_final(TabNet嵌入32维) = 62维
        hybrid_final_lgb = np.concatenate([wo_final, fe_final], axis=1)
        hybrid_test_final_lgb = np.concatenate([wo_test_final, fe_test_final], axis=1)
        
        # 特征选择并保存选中的特征名称
        selected_feature_names_lgb = lgb_feature_names.copy()
        if hybrid_final_lgb.shape[1] > feature_select_k:
            selector_lgb_final = SelectKBest(f_classif, k=feature_select_k)
            hybrid_final_lgb = selector_lgb_final.fit_transform(hybrid_final_lgb, y_train_final)
            hybrid_test_final_lgb = selector_lgb_final.transform(hybrid_test_final_lgb)
            # 获取选中特征的索引和名称
            selected_indices_lgb = selector_lgb_final.get_support(indices=True)
            selected_feature_names_lgb = [lgb_feature_names[i] for i in selected_indices_lgb]
        
        final_lgb.fit(hybrid_final_lgb, y_train_final)
        
        # LightGBM SHAP 分析
        explainer_lgb = shap.TreeExplainer(final_lgb)
        shap_values_lgb = explainer_lgb.shap_values(hybrid_test_final_lgb[:20])  # 只分析前20个样本
        
        # 对于二分类，LightGBM可能返回单个数组或两个数组
        if isinstance(shap_values_lgb, list):
            shap_values_lgb = shap_values_lgb[1]  # 使用正类的SHAP值
        
        print(f"\nLightGBM Model - Top 10 Important Features (by mean |SHAP|):")
        feature_importance_lgb = np.mean(np.abs(shap_values_lgb), axis=0)
        top_features_idx_lgb = np.argsort(feature_importance_lgb)[-10:]
        for i, idx in enumerate(reversed(top_features_idx_lgb)):
            feature_name = selected_feature_names_lgb[idx] if idx < len(selected_feature_names_lgb) else f"Feature_{idx}"
            print(f"  {i+1:2d}. {feature_name}: {feature_importance_lgb[idx]:.4f}")
        
        # 保存LightGBM SHAP图
        plt.figure(figsize=(14, 10))
        shap.summary_plot(shap_values_lgb, hybrid_test_final_lgb[:20], 
                         feature_names=selected_feature_names_lgb,
                         max_display=15, show=False)
        plt.title(f'LightGBM SHAP Analysis - Fold {fold}\n(Feature Set V2: Attention-Weighted Features + TabNet Embeddings)')
        plt.tight_layout()
        plt.savefig(f'lightgbm_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"LightGBM SHAP analysis failed: {e}")
    
    
    
    print(f"\n SHAP analysis completed for Fold {fold}")
    print(f"Thr={best_thr:.3f} | Acc={acc:.4f} | Sens={sens:.4f} | Spec={spec:.4f} | F1={f1:.4f} | Youden_J={best_youden_j:.4f}")
    results_list.append({
        "Acc": acc,
        "Sens": sens,
        "Spec": spec,
        "F1": f1,
        "Youden_J": best_youden_j,
        "Best_thr": best_thr
    })
    fold += 1

# ============== 5) 汇总结果 ==============
results_df = pd.DataFrame(results_list)
print("\n===== 汇总结果 =====")
print(results_df)
print("\n平均结果：")
print(results_df.mean(numeric_only=True))



