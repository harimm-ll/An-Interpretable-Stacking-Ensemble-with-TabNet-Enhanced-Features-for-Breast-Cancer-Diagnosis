import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from pytorch_tabnet.tab_model import TabNetClassifier
import torch
import math
import warnings
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# ============== 1) åŠ è½½ WDBC æ•°æ®é›† ==============
columns = [
    'ID', 'Diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',
    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',
    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
    'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',
    'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',
    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',
    'fractal_dimension_worst'
]

file_path = r"E:\google\Dataset\breast cancer\breast cancer\wdbc.data"
df = pd.read_csv(file_path, header=None, names=columns)

df['Diagnosis'] = LabelEncoder().fit_transform(df['Diagnosis'])
X = df.drop(['ID', 'Diagnosis'], axis=1).values
y = df['Diagnosis'].values

# æ ‡å‡†åŒ–
scaler = RobustScaler()
X = scaler.fit_transform(X)

# ============== 2) å‚æ•°è®¾ç½® ==============
SEED = 42
n_splits = 5  # å¤–å±‚äº¤å‰éªŒè¯çš„æŠ˜æ•°
torch.manual_seed(SEED)
np.random.seed(SEED)
outer_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)

# ç‰¹å¾å·¥ç¨‹å‚æ•°
pca_components = 8  # PCAé™ç»´åçš„ç»´åº¦
feature_select_k = 101  # ç‰¹å¾é€‰æ‹©ä¿ç•™çš„ç‰¹å¾æ•°é‡

results_list = []
fold = 1

# ============== 3) æ·±åº¦ç‰¹å¾æå–å‡½æ•° ==============
def extract_deep_features(tabnet_model, X_data):
    tabnet_model.network.eval()
    with torch.no_grad():
        X_tensor = torch.tensor(X_data).float()
        output, M_loss = tabnet_model.network.forward_masks(X_tensor)
        final_embedding = output.cpu().numpy()
        feature_importance = np.var(final_embedding, axis=0)
        feature_importance = feature_importance / np.sum(feature_importance)
        n_features = X_data.shape[1]
        if len(feature_importance) < n_features:
            repeat_times = n_features // len(feature_importance) + 1
            attention_weights = np.tile(feature_importance, repeat_times)[:n_features]
        else:
            attention_weights = feature_importance[:n_features]
        weighted_original = X_data * attention_weights
        multi_view_features = []
        for i in range(tabnet_model.n_steps):
            view_weights = attention_weights * (i + 1) / tabnet_model.n_steps
            view_features = X_data * view_weights
            multi_view_features.append(view_features)
        all_steps_features = np.concatenate(multi_view_features, axis=1)
        

        
    return final_embedding, all_steps_features, attention_weights, weighted_original

# ============== 4) å¤–å±‚äº¤å‰éªŒè¯å¾ªç¯ ==============
for train_idx, test_idx in outer_kf.split(X, y):
    print(f"\n===== Fold {fold} =====")
    X_train_full, X_test = X[train_idx], X[test_idx]
    y_train_full, y_test = y[train_idx], y[test_idx]

    # ========== å†…å±‚ OOF stacking ==========
    inner_kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)
    oof_preds = []
    oof_targets = []
    test_preds = []

    for inner_fold, (inner_tr_idx, inner_va_idx) in enumerate(inner_kf.split(X_train_full, y_train_full)):
        X_tr, X_va = X_train_full[inner_tr_idx], X_train_full[inner_va_idx]
        y_tr, y_va = y_train_full[inner_tr_idx], y_train_full[inner_va_idx]

        # ---- TabNet ----
        batch_size = 128
        steps_per_epoch = max(1, math.ceil(X_tr.shape[0] / batch_size))
        tabnet = TabNetClassifier(
            n_d=32, n_a=32, n_steps=4,
            gamma=1.8,
            lambda_sparse=5e-4,
            clip_value=2.0,
            mask_type="entmax",
            optimizer_fn=torch.optim.AdamW,
            optimizer_params=dict(lr=1e-3, weight_decay=1e-4),
            scheduler_params={"max_lr": 3e-3, "epochs": 200, "steps_per_epoch": steps_per_epoch, "pct_start": 0.2},
            scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,
            seed=SEED, verbose=0
        )
        tabnet.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            eval_name=["valid"],
            eval_metric=["auc"],
            patience=20,
            max_epochs=500,
            batch_size=batch_size,
            virtual_batch_size=64,
            num_workers=0,
            drop_last=False
        )

        # ---- ç‰¹å¾æå–ä¸èåˆ ----
        fe_tr, st_tr, aw_tr, wo_tr = extract_deep_features(tabnet, X_tr)
        fe_va, st_va, aw_va, wo_va = extract_deep_features(tabnet, X_va)
        fe_te, st_te, aw_te, wo_te = extract_deep_features(tabnet, X_test)

        # é™åˆ¶PCAç»´åº¦
        pca = PCA(n_components=pca_components, random_state=SEED)
        st_tr_pca = pca.fit_transform(st_tr)
        st_va_pca = pca.transform(st_va)
        st_te_pca = pca.transform(st_te)

        # ä¸ºXGBoostæ„å»ºç‰¹å¾é›†V1: åŸå§‹ç‰¹å¾ + æœ€ç»ˆembedding + æ‰€æœ‰æ­¥éª¤ç‰¹å¾
        hybrid_tr_xgb = np.concatenate([X_tr, fe_tr, st_tr_pca], axis=1)
        hybrid_va_xgb = np.concatenate([X_va, fe_va, st_va_pca], axis=1)
        hybrid_te_xgb = np.concatenate([X_test, fe_te, st_te_pca], axis=1)
        
        # é™åˆ¶XGBoostç‰¹å¾ç»´åº¦
        if hybrid_tr_xgb.shape[1] > feature_select_k:
            from sklearn.feature_selection import SelectKBest, f_classif
            selector_xgb = SelectKBest(f_classif, k=feature_select_k)
            hybrid_tr_xgb = selector_xgb.fit_transform(hybrid_tr_xgb, y_tr)
            hybrid_va_xgb = selector_xgb.transform(hybrid_va_xgb)
            hybrid_te_xgb = selector_xgb.transform(hybrid_te_xgb)
        
        # ä¸ºLightGBMæ„å»ºç‰¹å¾é›†V2: æ³¨æ„åŠ›åŠ æƒç‰¹å¾ + embedding
        hybrid_tr_lgb = np.concatenate([wo_tr, fe_tr], axis=1)
        hybrid_va_lgb = np.concatenate([wo_va, fe_va], axis=1)
        hybrid_te_lgb = np.concatenate([wo_te, fe_te], axis=1)
        


        # ---- XGB (ä½¿ç”¨ç‰¹å¾é›†V1) ----
        xgb_clf = xgb.XGBClassifier(
            n_estimators=569,  # é™ä½n_estimators
            max_depth=5,
            learning_rate=0.34155,
            subsample=0.74263,
            colsample_bytree=0.738128,
            random_state=SEED,
            eval_metric='auc',
            early_stopping_rounds=30  # è°ƒæ•´æ—©åœå‚æ•°
        )
        xgb_clf.fit(hybrid_tr_xgb, y_tr, eval_set=[(hybrid_va_xgb, y_va)], verbose=False)
        val_proba_xgb = xgb_clf.predict_proba(hybrid_va_xgb)[:, 1]
        test_proba_xgb = xgb_clf.predict_proba(hybrid_te_xgb)[:, 1]

        # ---- LGB (ä½¿ç”¨ç‰¹å¾é›†V2) ----
        lgb_train = lgb.Dataset(hybrid_tr_lgb, label=y_tr)
        lgb_val_dataset = lgb.Dataset(hybrid_va_lgb, label=y_va, reference=lgb_train)
        params = {
            "objective": "binary",
            "boosting_type": "gbdt",
            "metric": "auc",
            "num_leaves": 146,
            "learning_rate": 0.0773,
            "subsample": 0.7682,
            "colsample_bytree": 0.77594,
            "reg_alpha": 0.6171,
            "reg_lambda": 0.2901,
            "min_child_weight": 4,
            "random_state": SEED,
            "verbose": -1
        }
        lgb_clf = lgb.train(
            params,
            lgb_train,
            valid_sets=[lgb_val_dataset],
            num_boost_round=2000,
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        val_proba_lgb = lgb_clf.predict(hybrid_va_lgb, num_iteration=lgb_clf.best_iteration)
        test_proba_lgb = lgb_clf.predict(hybrid_te_lgb, num_iteration=lgb_clf.best_iteration)

        # ---- ä¿å­˜ OOF ----
        stack_val = np.vstack([val_proba_xgb, val_proba_lgb]).T
        stack_test = np.vstack([test_proba_xgb, test_proba_lgb]).T

        oof_preds.append(stack_val)
        oof_targets.append(y_va)
        test_preds.append(stack_test)
        
        # ä¿å­˜æœ€åä¸€æŠ˜çš„ç‰¹å¾ä¿¡æ¯ç”¨äºåç»­åˆ†æ
        if inner_fold == n_splits - 1:
            last_fold_features = {
                'fe_te': fe_te, 'st_te': st_te, 'aw_te': aw_te, 'wo_te': wo_te,
                'st_te_pca': st_te_pca, 'hybrid_te_xgb': hybrid_te_xgb, 
                'hybrid_te_lgb': hybrid_te_lgb, 'X_test': X_test
            }

    # ç¡®ä¿last_fold_featureså·²å®šä¹‰
    if 'last_fold_features' not in locals():
        # å¦‚æœæ²¡æœ‰æ‰§è¡Œåˆ°æœ€åä¸€æŠ˜ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„last_fold_features
        last_fold_features = {
            'fe_te': fe_te, 'st_te': st_te, 'aw_te': aw_te, 'wo_te': wo_te,
            'st_te_pca': st_te_pca, 'hybrid_te_xgb': hybrid_te_xgb, 
            'hybrid_te_lgb': hybrid_te_lgb, 'X_test': X_test
        }
    
    # ====== Meta æ¨¡å‹è®­ç»ƒ ======
    oof_preds = np.vstack(oof_preds)
    oof_targets = np.concatenate(oof_targets)
    test_preds = np.mean(test_preds, axis=0)

    meta_clf = LogisticRegression(random_state=SEED, solver="liblinear")
    meta_clf.fit(oof_preds, oof_targets)

    y_test_proba = meta_clf.predict_proba(test_preds)[:, 1]
    thresholds = np.linspace(0.1, 0.9, 81)
    best_youden_j = 0
    best_thr = 0.5
    for thr in thresholds:
        y_pred = (y_test_proba >= thr).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        youden_j = sens + spec - 1
        if youden_j > best_youden_j:
            best_youden_j = youden_j
            best_thr = thr

    y_pred_final = (y_test_proba >= best_thr).astype(int)
    acc = accuracy_score(y_test, y_pred_final)
    f1 = f1_score(y_test, y_pred_final)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final).ravel()
    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # ============== ç‰¹å¾ä½¿ç”¨æƒ…å†µåˆ†æ ==============
    print(f"\n=== Fold {fold} - TabNetç‰¹å¾æå–ä¸æ¨¡å‹ç‰¹å¾é›†åˆ†æ ===")
    print(f"åŸºäºå†…å±‚äº¤å‰éªŒè¯æœ€åä¸€æŠ˜çš„ç‰¹å¾æ„æˆ:")
    
    # æ˜¾ç¤ºTabNetè¾“å‡ºçš„ä¸‰å±‚ç‰¹å¾ä¿¡æ¯
    print(f"\n1. TabNetä¸‰å±‚ç‰¹å¾è¾“å‡º:")
    print(f"   - æœ€ç»ˆåµŒå…¥ç‰¹å¾ (final_embedding): {last_fold_features['fe_te'].shape} -> ç”¨äºXGBå’ŒLGB")
    print(f"   - å¤šæ­¥éª¤ç‰¹å¾ (all_steps_features): {last_fold_features['st_te'].shape} -> ç»PCAé™ç»´åç”¨äºXGB")
    print(f"   - æ³¨æ„åŠ›æƒé‡ (attention_weights): {last_fold_features['aw_te'].shape} -> ç”¨äºç”ŸæˆåŠ æƒç‰¹å¾")
    print(f"   - åŠ æƒåŸå§‹ç‰¹å¾ (weighted_original): {last_fold_features['wo_te'].shape} -> ç”¨äºLGB")
    
    # æ˜¾ç¤ºåç»­æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾é›†
    print(f"\n2. XGBoostç‰¹å¾é›†V1æ„æˆ (æ€»ç»´åº¦: {last_fold_features['hybrid_te_xgb'].shape[1]}):")
    print(f"   - åŸå§‹ç‰¹å¾: {last_fold_features['X_test'].shape[1]}ç»´")
    print(f"   - TabNetåµŒå…¥: {last_fold_features['fe_te'].shape[1]}ç»´")
    print(f"   - PCAé™ç»´æ­¥éª¤ç‰¹å¾: {last_fold_features['st_te_pca'].shape[1]}ç»´")
    if last_fold_features['hybrid_te_xgb'].shape[1] != (last_fold_features['X_test'].shape[1] + last_fold_features['fe_te'].shape[1] + last_fold_features['st_te_pca'].shape[1]):
        print(f"   - ç‰¹å¾é€‰æ‹©å: {last_fold_features['hybrid_te_xgb'].shape[1]}ç»´ (SelectKBestä¿ç•™å‰{feature_select_k}ä¸ª)")
    
    print(f"\n3. LightGBMç‰¹å¾é›†V2æ„æˆ (æ€»ç»´åº¦: {last_fold_features['hybrid_te_lgb'].shape[1]}):")
    print(f"   - æ³¨æ„åŠ›åŠ æƒç‰¹å¾: {last_fold_features['wo_te'].shape[1]}ç»´")
    print(f"   - TabNetåµŒå…¥: {last_fold_features['fe_te'].shape[1]}ç»´")
    print(f"   - æ— éœ€ç‰¹å¾é€‰æ‹© (ç»´åº¦ < {feature_select_k})")
    
    print(f"\n4. æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ:")
    print(f"   - æƒé‡èŒƒå›´: [{last_fold_features['aw_te'].min():.6f}, {last_fold_features['aw_te'].max():.6f}]")
    print(f"   - å‰10ä¸ªç‰¹å¾æƒé‡: {last_fold_features['aw_te'][:10]}")
    
    # ============== SHAP åˆ†æ ==============
    print(f"\n--- SHAP Analysis for Fold {fold} ---")
    print(f"åˆ†æè¯´æ˜: ä½¿ç”¨å†…éƒ¨äº¤å‰éªŒè¯æœ€åä¸€ä¸ªfoldè®­ç»ƒçš„å®é™…æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ")
    
    # å®šä¹‰åŸå§‹ç‰¹å¾åç§°
    original_feature_names = [
        'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
        'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',
        'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',
        'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
        'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',
        'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',
        'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',
        'fractal_dimension_worst'
    ]
    
    # 1. å…ƒæ¨¡å‹ SHAP åˆ†æ
    try:
        # ä½¿ç”¨çº¿æ€§è§£é‡Šå™¨åˆ†æå…ƒæ¨¡å‹ï¼ˆLogisticRegressionï¼‰
        explainer_meta = shap.LinearExplainer(meta_clf, oof_preds)
        shap_values_meta = explainer_meta.shap_values(test_preds)
        
        print(f"Meta Model SHAP - Base Model Contributions:")
        print(f"  XGBoost contribution: {np.mean(np.abs(shap_values_meta[:, 0])):.4f}")
        print(f"  LightGBM contribution: {np.mean(np.abs(shap_values_meta[:, 1])):.4f}")
        
        # ä¿å­˜å…ƒæ¨¡å‹SHAPå›¾
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values_meta, test_preds, 
                         feature_names=['XGBoost_Probability', 'LightGBM_Probability'], 
                         show=False)
        plt.title(f'Meta Model SHAP Analysis - Fold {fold}\n(Stacking Ensemble Decision Process)')
        plt.tight_layout()
        plt.savefig(f'meta_model_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"Meta model SHAP analysis failed: {e}")
    
    # 2. XGBoost SHAP åˆ†æï¼ˆä½¿ç”¨å†…éƒ¨å¾ªç¯æœ€åä¸€ä¸ªfoldçš„å®é™…æ¨¡å‹ï¼‰
    try:
        print(f"\n=== XGBoost Model Analysis (Inner CV Last Fold) ===")
        print(f"æ¨¡å‹é…ç½®: n_estimators=569, max_depth=5, learning_rate=0.34155")
        print(f"ç‰¹å¾é›†V1: åŸå§‹ç‰¹å¾(30) + TabNetåµŒå…¥(32) + PCAé™ç»´æ­¥éª¤ç‰¹å¾(8) = æ€»è®¡70ç»´")
        print(f"ç‰¹å¾é€‰æ‹©: ä½¿ç”¨SelectKBestä¿ç•™å‰{feature_select_k}ä¸ªæœ€é‡è¦ç‰¹å¾")
        
        # ä½¿ç”¨å†…éƒ¨å¾ªç¯æœ€åä¸€ä¸ªfoldçš„æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ
        # é‡æ–°è®­ç»ƒä»¥è·å¾—ä¸å†…éƒ¨å¾ªç¯ç›¸åŒçš„æ¨¡å‹é…ç½®
        final_xgb = xgb.XGBClassifier(
            n_estimators=569, max_depth=5, learning_rate=0.34155,
            subsample=0.74263, colsample_bytree=0.738128,
            random_state=SEED, eval_metric='auc'
        )
        
        # ä½¿ç”¨å®Œæ•´è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆæ¨¡æ‹Ÿå†…éƒ¨å¾ªç¯æœ€åä¸€ä¸ªfoldï¼‰
        X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
            X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=SEED
        )
        
        # é‡æ–°æå–ç‰¹å¾ç”¨äºSHAPåˆ†æ
        tabnet_final = TabNetClassifier(
            n_d=32, n_a=32, n_steps=4, gamma=1.8, lambda_sparse=5e-4,
            clip_value=2.0, mask_type="entmax", optimizer_fn=torch.optim.AdamW,
            optimizer_params=dict(lr=1e-3, weight_decay=1e-4),
            seed=SEED, verbose=0
        )
        tabnet_final.fit(X_train_final, y_train_final, eval_set=[(X_val_final, y_val_final)],
                        eval_name=["valid"], eval_metric=["auc"], patience=20,
                        max_epochs=200, batch_size=128, virtual_batch_size=64,
                        num_workers=0, drop_last=False)
        
        fe_final, st_final, _, _ = extract_deep_features(tabnet_final, X_train_final)
        fe_test_final, st_test_final, _, _ = extract_deep_features(tabnet_final, X_test)
        
        pca_final = PCA(n_components=pca_components, random_state=SEED)
        st_final_pca = pca_final.fit_transform(st_final)
        st_test_final_pca = pca_final.transform(st_test_final)
        
        # æ„å»ºXGBoostç‰¹å¾é›†V1çš„ç‰¹å¾åç§°
        tabnet_embedding_names = [f'TabNet_Embed_{i+1}' for i in range(fe_final.shape[1])]
        pca_step_names = [f'TabNet_PCA_Step_{i+1}' for i in range(st_final_pca.shape[1])]
        xgb_feature_names = original_feature_names + tabnet_embedding_names + pca_step_names
        
        hybrid_final_xgb = np.concatenate([X_train_final, fe_final, st_final_pca], axis=1)
        hybrid_test_final_xgb = np.concatenate([X_test, fe_test_final, st_test_final_pca], axis=1)
        
        # ç‰¹å¾é€‰æ‹©å¹¶ä¿å­˜é€‰ä¸­çš„ç‰¹å¾åç§°
        selected_feature_names = xgb_feature_names.copy()
        if hybrid_final_xgb.shape[1] > feature_select_k:
            from sklearn.feature_selection import SelectKBest, f_classif
            selector_final = SelectKBest(f_classif, k=feature_select_k)
            hybrid_final_xgb = selector_final.fit_transform(hybrid_final_xgb, y_train_final)
            hybrid_test_final_xgb = selector_final.transform(hybrid_test_final_xgb)
            # è·å–é€‰ä¸­ç‰¹å¾çš„ç´¢å¼•å’Œåç§°
            selected_indices = selector_final.get_support(indices=True)
            selected_feature_names = [xgb_feature_names[i] for i in selected_indices]
        
        final_xgb.fit(hybrid_final_xgb, y_train_final)
        
        # XGBoost SHAP åˆ†æ
        explainer_xgb = shap.TreeExplainer(final_xgb)
        shap_values_xgb = explainer_xgb.shap_values(hybrid_test_final_xgb[:20])  # åªåˆ†æå‰20ä¸ªæ ·æœ¬ä»¥èŠ‚çœæ—¶é—´
        
        print(f"\nXGBoost Model - Top 10 Important Features (by mean |SHAP|):")
        feature_importance_xgb = np.mean(np.abs(shap_values_xgb), axis=0)
        top_features_idx = np.argsort(feature_importance_xgb)[-10:]
        for i, idx in enumerate(reversed(top_features_idx)):
            feature_name = selected_feature_names[idx] if idx < len(selected_feature_names) else f"Feature_{idx}"
            print(f"  {i+1:2d}. {feature_name}: {feature_importance_xgb[idx]:.4f}")
        
        # ä¿å­˜XGBoost SHAPå›¾
        plt.figure(figsize=(14, 10))
        shap.summary_plot(shap_values_xgb, hybrid_test_final_xgb[:20], 
                         feature_names=selected_feature_names,
                         max_display=15, show=False)
        plt.title(f'XGBoost SHAP Analysis - Fold {fold}\n(Feature Set V1: Original + TabNet Embeddings + PCA Steps)')
        plt.tight_layout()
        plt.savefig(f'xgboost_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"XGBoost SHAP analysis failed: {e}")
    
    # 3. LightGBM SHAP åˆ†æï¼ˆä½¿ç”¨å†…éƒ¨å¾ªç¯æœ€åä¸€ä¸ªfoldçš„å®é™…æ¨¡å‹ï¼‰
    try:
        print(f"\n=== LightGBM Model Analysis (Inner CV Last Fold) ===")
        print(f"æ¨¡å‹é…ç½®: num_leaves=146, learning_rate=0.0773, subsample=0.7682")
        print(f"ç‰¹å¾é›†V2: æ³¨æ„åŠ›åŠ æƒç‰¹å¾(30) + TabNetåµŒå…¥(32) = æ€»è®¡62ç»´")
        print(f"ç‰¹å¾é€‰æ‹©: ä½¿ç”¨SelectKBestä¿ç•™å‰{feature_select_k}ä¸ªæœ€é‡è¦ç‰¹å¾ï¼ˆå¦‚æœè¶…è¿‡62ç»´ï¼‰")
        
        # ä½¿ç”¨ä¸å†…éƒ¨å¾ªç¯ç›¸åŒçš„LightGBMå‚æ•°é…ç½®
        final_lgb = lgb.LGBMClassifier(
            num_leaves=146, learning_rate=0.0773, subsample=0.7682,
            colsample_bytree=0.77594, reg_alpha=0.6171, reg_lambda=0.2901,
            min_child_weight=4, random_state=SEED, objective='binary', 
            metric='auc', verbose=-1, n_estimators=2000
        )
        
        # æ„å»ºLightGBMç‰¹å¾é›†V2çš„ç‰¹å¾åç§°ï¼ˆæ³¨æ„ï¼šä½¿ç”¨æ³¨æ„åŠ›åŠ æƒç‰¹å¾è€ŒéåŸå§‹ç‰¹å¾ï¼‰
        _, _, aw_final, wo_final = extract_deep_features(tabnet_final, X_train_final)
        _, _, aw_test_final, wo_test_final = extract_deep_features(tabnet_final, X_test)
        
        # ä¸ºLightGBMæ„å»ºç‰¹å¾é›†V2: æ³¨æ„åŠ›åŠ æƒç‰¹å¾ + embeddingï¼ˆä¸stackingä¸­å®Œå…¨ä¸€è‡´ï¼‰
        weighted_feature_names = [f'Weighted_{name}' for name in original_feature_names]
        tabnet_embedding_names_lgb = [f'TabNet_Embed_{i+1}' for i in range(fe_final.shape[1])]
        lgb_feature_names = weighted_feature_names + tabnet_embedding_names_lgb
        
        # ç‰¹å¾é›†V2æ„å»ºï¼šwo_final(æ³¨æ„åŠ›åŠ æƒç‰¹å¾30ç»´) + fe_final(TabNetåµŒå…¥32ç»´) = 62ç»´
        hybrid_final_lgb = np.concatenate([wo_final, fe_final], axis=1)
        hybrid_test_final_lgb = np.concatenate([wo_test_final, fe_test_final], axis=1)
        
        # ç‰¹å¾é€‰æ‹©å¹¶ä¿å­˜é€‰ä¸­çš„ç‰¹å¾åç§°
        selected_feature_names_lgb = lgb_feature_names.copy()
        if hybrid_final_lgb.shape[1] > feature_select_k:
            selector_lgb_final = SelectKBest(f_classif, k=feature_select_k)
            hybrid_final_lgb = selector_lgb_final.fit_transform(hybrid_final_lgb, y_train_final)
            hybrid_test_final_lgb = selector_lgb_final.transform(hybrid_test_final_lgb)
            # è·å–é€‰ä¸­ç‰¹å¾çš„ç´¢å¼•å’Œåç§°
            selected_indices_lgb = selector_lgb_final.get_support(indices=True)
            selected_feature_names_lgb = [lgb_feature_names[i] for i in selected_indices_lgb]
        
        final_lgb.fit(hybrid_final_lgb, y_train_final)
        
        # LightGBM SHAP åˆ†æ
        explainer_lgb = shap.TreeExplainer(final_lgb)
        shap_values_lgb = explainer_lgb.shap_values(hybrid_test_final_lgb[:20])  # åªåˆ†æå‰20ä¸ªæ ·æœ¬
        
        # å¯¹äºäºŒåˆ†ç±»ï¼ŒLightGBMå¯èƒ½è¿”å›å•ä¸ªæ•°ç»„æˆ–ä¸¤ä¸ªæ•°ç»„
        if isinstance(shap_values_lgb, list):
            shap_values_lgb = shap_values_lgb[1]  # ä½¿ç”¨æ­£ç±»çš„SHAPå€¼
        
        print(f"\nLightGBM Model - Top 10 Important Features (by mean |SHAP|):")
        feature_importance_lgb = np.mean(np.abs(shap_values_lgb), axis=0)
        top_features_idx_lgb = np.argsort(feature_importance_lgb)[-10:]
        for i, idx in enumerate(reversed(top_features_idx_lgb)):
            feature_name = selected_feature_names_lgb[idx] if idx < len(selected_feature_names_lgb) else f"Feature_{idx}"
            print(f"  {i+1:2d}. {feature_name}: {feature_importance_lgb[idx]:.4f}")
        
        # ä¿å­˜LightGBM SHAPå›¾
        plt.figure(figsize=(14, 10))
        shap.summary_plot(shap_values_lgb, hybrid_test_final_lgb[:20], 
                         feature_names=selected_feature_names_lgb,
                         max_display=15, show=False)
        plt.title(f'LightGBM SHAP Analysis - Fold {fold}\n(Feature Set V2: Attention-Weighted Features + TabNet Embeddings)')
        plt.tight_layout()
        plt.savefig(f'lightgbm_shap_fold_{fold}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"LightGBM SHAP analysis failed: {e}")
    
    # 4. æ¨¡å‹æ¶æ„ä¸SHAPåˆ†ææ€»ç»“
    print(f"\n=== Model Architecture Summary for Fold {fold} ===")
    print(f"\nğŸ—ï¸  æ•´ä½“æ¶æ„:")
    print(f"   â”œâ”€â”€ å¤–å±‚5æŠ˜äº¤å‰éªŒè¯ (å½“å‰: Fold {fold})")
    print(f"   â”œâ”€â”€ å†…å±‚4æŠ˜äº¤å‰éªŒè¯ (ç”¨äºOOF Stacking)")
    print(f"   â””â”€â”€ ä¸‰å±‚æ¨¡å‹é›†æˆæ¶æ„")
    
    print(f"\nğŸ§  ç¬¬ä¸€å±‚: TabNetæ·±åº¦ç‰¹å¾æå–å™¨")
    print(f"   â”œâ”€â”€ æ¶æ„: n_d=32, n_a=32, n_steps=4")
    print(f"   â”œâ”€â”€ è¾“å‡º1: æœ€ç»ˆåµŒå…¥ç‰¹å¾ (32ç»´)")
    print(f"   â”œâ”€â”€ è¾“å‡º2: å¤šæ­¥éª¤ç‰¹å¾ (4Ã—32=128ç»´)")
    print(f"   â””â”€â”€ è¾“å‡º3: æ³¨æ„åŠ›åŠ æƒåŸå§‹ç‰¹å¾ (30ç»´)")
    
    print(f"\nğŸŒ³ ç¬¬äºŒå±‚: åŒè·¯å¾„åŸºå­¦ä¹ å™¨")
    print(f"   â”œâ”€â”€ XGBoostè·¯å¾„ (ç‰¹å¾é›†V1):")
    print(f"   â”‚   â”œâ”€â”€ è¾“å…¥: åŸå§‹ç‰¹å¾(30) + TabNetåµŒå…¥(32) + PCAæ­¥éª¤ç‰¹å¾(8)")
    print(f"   â”‚   â”œâ”€â”€ ç‰¹å¾é€‰æ‹©: SelectKBestä¿ç•™{feature_select_k}ä¸ªç‰¹å¾")
    print(f"   â”‚   â””â”€â”€ å‚æ•°: n_estimators=569, max_depth=5, lr=0.34155")
    print(f"   â””â”€â”€ LightGBMè·¯å¾„ (ç‰¹å¾é›†V2):")
    print(f"       â”œâ”€â”€ è¾“å…¥: æ³¨æ„åŠ›åŠ æƒç‰¹å¾(30) + TabNetåµŒå…¥(32) = 62ç»´")
    print(f"       â”œâ”€â”€ ç‰¹å¾é€‰æ‹©: æ— éœ€é€‰æ‹©ï¼ˆç‰¹å¾æ•°62 < {feature_select_k}ï¼‰")
    print(f"       â””â”€â”€ å‚æ•°: num_leaves=146, lr=0.0773, subsample=0.7682")
    
    print(f"\nğŸ¯ ç¬¬ä¸‰å±‚: å…ƒå­¦ä¹ å™¨ (Stacking)")
    print(f"   â”œâ”€â”€ æ¨¡å‹: LogisticRegression")
    print(f"   â”œâ”€â”€ è¾“å…¥: XGBoostæ¦‚ç‡ + LightGBMæ¦‚ç‡")
    print(f"   â””â”€â”€ è¾“å‡º: æœ€ç»ˆé¢„æµ‹æ¦‚ç‡")
    
    print(f"\nğŸ“Š SHAPåˆ†æè¯´æ˜:")
    print(f"   â”œâ”€â”€ å…ƒæ¨¡å‹SHAP: åˆ†æXGBoost vs LightGBMçš„è´¡çŒ®æƒé‡")
    print(f"   â”œâ”€â”€ XGBoost SHAP: åˆ†ææ··åˆç‰¹å¾é›†V1ä¸­å„ç‰¹å¾çš„é‡è¦æ€§")
    print(f"   â”œâ”€â”€ LightGBM SHAP: åˆ†ææ··åˆç‰¹å¾é›†V2ä¸­å„ç‰¹å¾çš„é‡è¦æ€§")
    print(f"   â””â”€â”€ ç‰¹å¾å‘½å: åŒ…å«åŸå§‹ç‰¹å¾åã€TabNetåµŒå…¥ã€PCAç»„ä»¶ç­‰")
    
    print(f"\nâœ… SHAP analysis completed for Fold {fold}")
    print(f"Thr={best_thr:.3f} | Acc={acc:.4f} | Sens={sens:.4f} | Spec={spec:.4f} | F1={f1:.4f} | Youden_J={best_youden_j:.4f}")
    results_list.append({
        "Acc": acc,
        "Sens": sens,
        "Spec": spec,
        "F1": f1,
        "Youden_J": best_youden_j,
        "Best_thr": best_thr
    })
    fold += 1

# ============== 5) æ±‡æ€»ç»“æœ ==============
results_df = pd.DataFrame(results_list)
print("\n===== æ±‡æ€»ç»“æœ =====")
print(results_df)
print("\nå¹³å‡ç»“æœï¼š")
print(results_df.mean(numeric_only=True))

# ============== æœ€ç»ˆ SHAP æ±‡æ€»åˆ†æ ==============
print("\n" + "="*80)
print("ğŸ¯ FINAL SHAP ANALYSIS SUMMARY")
print("="*80)
print(f"\nğŸ“ ç”Ÿæˆçš„SHAPå¯è§†åŒ–æ–‡ä»¶ (å…±{n_splits * 3}ä¸ª):")
for fold_num in range(1, n_splits + 1):
    print(f"\n  ğŸ“Š Fold {fold_num}:")
    print(f"    â”œâ”€â”€ meta_model_shap_fold_{fold_num}.png")
    print(f"    â”‚   â””â”€â”€ å…ƒæ¨¡å‹å†³ç­–è¿‡ç¨‹: XGBoost vs LightGBMè´¡çŒ®æƒé‡")
    print(f"    â”œâ”€â”€ xgboost_shap_fold_{fold_num}.png")
    print(f"    â”‚   â””â”€â”€ XGBoostç‰¹å¾é‡è¦æ€§: æ··åˆç‰¹å¾é›†V1 (åŸå§‹+åµŒå…¥+PCA)")
    print(f"    â””â”€â”€ lightgbm_shap_fold_{fold_num}.png")
    print(f"        â””â”€â”€ LightGBMç‰¹å¾é‡è¦æ€§: æ··åˆç‰¹å¾é›†V2 (åŠ æƒ+åµŒå…¥)")

print(f"\nğŸ” SHAPåˆ†ææ·±åº¦è§£è¯»:")
print(f"\n1ï¸âƒ£  å…ƒæ¨¡å‹å±‚é¢ (Meta-Model Level):")
print(f"   ğŸ¯ ç›®æ ‡: ç†è§£Stackingé›†æˆä¸­åŸºæ¨¡å‹çš„æƒé‡åˆ†é…")
print(f"   ğŸ“Š æ–¹æ³•: LinearExplaineråˆ†æLogisticRegression")
print(f"   ğŸ’¡ æ´å¯Ÿ: æ­ç¤ºXGBoost vs LightGBMåœ¨æœ€ç»ˆå†³ç­–ä¸­çš„ç›¸å¯¹é‡è¦æ€§")

print(f"\n2ï¸âƒ£  XGBoostæ¨¡å‹å±‚é¢:")
print(f"   ğŸ¯ ç›®æ ‡: åˆ†æç‰¹å¾é›†V1ä¸­å„ç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®")
print(f"   ğŸ“Š æ–¹æ³•: TreeExplaineråˆ†æå†…éƒ¨CVæœ€åfoldçš„XGBoostæ¨¡å‹")
print(f"   ğŸ”§ ç‰¹å¾å·¥ç¨‹: åŸå§‹ç‰¹å¾(30) + TabNetåµŒå…¥(32) + PCAæ­¥éª¤ç‰¹å¾(8)")
print(f"   ğŸ›ï¸  ç‰¹å¾é€‰æ‹©: SelectKBestä¿ç•™å‰{feature_select_k}ä¸ªæœ€é‡è¦ç‰¹å¾")
print(f"   ğŸ’¡ æ´å¯Ÿ: è¯†åˆ«åœ¨æ¢¯åº¦æå‡æ¡†æ¶ä¸‹æœ€å…·é¢„æµ‹ä»·å€¼çš„æ··åˆç‰¹å¾")

print(f"\n3ï¸âƒ£  LightGBMæ¨¡å‹å±‚é¢:")
print(f"   ğŸ¯ ç›®æ ‡: åˆ†æç‰¹å¾é›†V2ä¸­å„ç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®")
print(f"   ğŸ“Š æ–¹æ³•: TreeExplaineråˆ†æå†…éƒ¨CVæœ€åfoldçš„LightGBMæ¨¡å‹")
print(f"   ğŸ”§ ç‰¹å¾å·¥ç¨‹: æ³¨æ„åŠ›åŠ æƒç‰¹å¾(30) + TabNetåµŒå…¥(32) = 62ç»´")
print(f"   ğŸ›ï¸  ç‰¹å¾é€‰æ‹©: ç”±äºç‰¹å¾æ•°(62) < {feature_select_k}ï¼Œæ— éœ€ç‰¹å¾é€‰æ‹©")
print(f"   ğŸ’¡ æ´å¯Ÿ: æ­ç¤ºæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾çš„é¢„æµ‹ä»·å€¼")

print(f"\nğŸ§  æŠ€æœ¯åˆ›æ–°ç‚¹:")
print(f"   âœ¨ å¤šå±‚æ¬¡ç‰¹å¾å‘½å: åŒºåˆ†åŸå§‹ç‰¹å¾ã€TabNetåµŒå…¥ã€PCAç»„ä»¶")
print(f"   âœ¨ çœŸå®æ¨¡å‹åˆ†æ: ä½¿ç”¨å†…éƒ¨CVå®é™…è®­ç»ƒçš„æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ")
print(f"   âœ¨ å·®å¼‚åŒ–ç‰¹å¾é›†: V1(PCAé™ç»´) vs V2(æ³¨æ„åŠ›åŠ æƒ)")
print(f"   âœ¨ å®Œæ•´è§£é‡Šé“¾: ä»ç‰¹å¾â†’åŸºæ¨¡å‹â†’å…ƒæ¨¡å‹çš„å…¨é“¾è·¯è§£é‡Š")

print(f"\nğŸ“ˆ åˆ†æä»·å€¼:")
print(f"   ğŸ”¬ æ¨¡å‹é€æ˜åº¦: æ·±åº¦ç†è§£å¤æ‚é›†æˆæ¨¡å‹çš„å†³ç­–æœºåˆ¶")
print(f"   ğŸ¯ ç‰¹å¾æ´å¯Ÿ: è¯†åˆ«ä¹³è…ºç™Œè¯Šæ–­ä¸­æœ€å…³é”®çš„ç”Ÿç‰©æ ‡å¿—ç‰©")
print(f"   âš–ï¸  æ¨¡å‹å¯¹æ¯”: é‡åŒ–ä¸åŒç®—æ³•åœ¨ç‰¹å¾åˆ©ç”¨ä¸Šçš„å·®å¼‚")
print(f"   ğŸ› ï¸  ä¼˜åŒ–æŒ‡å¯¼: ä¸ºæ¨¡å‹æ”¹è¿›å’Œç‰¹å¾å·¥ç¨‹æä¾›æ•°æ®é©±åŠ¨çš„å»ºè®®")

print(f"\n" + "="*80)
print(f"âœ… SHAP Analysis Pipeline Completed Successfully!")
print(f"ğŸ“Š Total Visualizations Generated: {n_splits * 3}")
print(f"ğŸ” Analysis Depth: 3-Layer Model Architecture")
print(f"ğŸ“ˆ Feature Coverage: {len(original_feature_names)} Original + TabNet Embeddings + Engineered Features")
print("="*80)
